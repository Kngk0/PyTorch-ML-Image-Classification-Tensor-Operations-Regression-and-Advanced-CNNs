{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069e89fa-8aa8-478c-8d9d-c100df43b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input and output vectors are given. \n",
    "inp = [ 0.7300, -1.0400, -1.2300,  1.6700, -0.6300,  1.4300, -0.8400,  0.1500,\n",
    "         -2.3000,  3.1000, -1.4500, -1.8100,  1.8700, -0.1100, -0.2800,  1.1200,\n",
    "         -0.4200,  2.8900]\n",
    "out = [ 1.43,  10.1,  8.3,  1.03,  10.21, -0.1,  8.92,  5.1,\n",
    "         -7.53, 34.72,  7.61,  3.2,  2.19,  7.15,  7.69, -0.18,\n",
    "          8.81, 23.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85cd7d2d-2354-4415-945e-706b1dafebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the polynomial model of degree 3, i.e., having 3 weights and 1 bias. \n",
    "# Also define the loss function\n",
    "def polynomial_model(x, weights, bias):\n",
    "    return weights[0] * x**3 + weights[1] * x**2 + weights[2] * x + bias\n",
    "\n",
    "def loss(y_predict, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff844417-2447-4cd8-92dc-92c608f3fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient manually wrt the exisiting parameters\n",
    "# Note: You need to define appropriate derivative functions to define the gradient\n",
    "# Use the defined gradient function to define the training function \n",
    "# Note: You cannot use autograd and optimizers\n",
    "# Run it on the input and output vector with appropriate learning rate and number of iterations\n",
    "# Plot the learned curve\n",
    "def gradient(x, y, weights, bias):\n",
    "    n = len(x)\n",
    "    grad_weights = np.zeros(3)\n",
    "    grad_bias = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        pred = polynomial_model(x[i], weights, bias)\n",
    "        grad_weights[0] += 2 * (pred - y[i]) * x[i]**3\n",
    "        grad_weights[1] += 2 * (pred - y[i]) * x[i]**2\n",
    "        grad_weights[2] += 2 * (pred - y[i]) * x[i]\n",
    "        grad_weights += 2 * (pred - y[i])\n",
    "\n",
    "    grad_weights /= n\n",
    "    grad_bias /= n\n",
    "\n",
    "    return grad_weights, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2268934b-6862-422d-9960-a9e30091edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Define the training function\n",
    "# Note: You cannot use optimizers.\n",
    "# Run it on the input and output vector with appropriate learning rate and number of iterations\n",
    "# Plot the learned curve\n",
    "def train(x, y, learning_rate, iterations):\n",
    "    weights = np.random.randn(3)\n",
    "    bias = np.random.randn()\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(iterations):\n",
    "        y_pred = polynomial_model(x, weights, bias)\n",
    "        loss = loss(y_pred, y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad_weights, grad_bias = gradient(x, y, weights, bias)\n",
    "        weights -= learning_rate * grad_bias\n",
    "\n",
    "    return weights, bias, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b8d657-4aef-42a0-806e-0f3fa5c8d4a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      7\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 8\u001b[0m final_weights, final_bias, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(x, y, learning_rate, iterations)\u001b[0m\n\u001b[1;32m     10\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpolynomial_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss(y_pred, y)\n\u001b[1;32m     14\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mpolynomial_model\u001b[0;34m(x, weights, bias)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolynomial_model\u001b[39m(x, weights, bias):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m \u001b[38;5;241m+\u001b[39m weight[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m weight[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m bias\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Use optimizers to abstract how parameters get updated\n",
    "# Define the training function\n",
    "# Run it on the input and output vector with appropriate learning rate, number of iterations, and SGD optimizer\n",
    "# Plot the learned curve\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "final_weights, final_bias, losses = train(inp, out, learning_rate, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961f2ff-d345-4ef6-98ab-047ed9f4411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to training set (80%) and validation set (20%) randomly\n",
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Use optimizers to abstract how parameters get updated\n",
    "# Define the training function that tracks both training and validation losses\n",
    "# Run it on the input and output vector with appropriate learning rate, number of iterations, and SGD optimizer\n",
    "# Plot the learned curve\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
