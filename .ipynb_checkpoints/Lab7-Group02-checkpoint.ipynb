{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9d0641-d627-4922-b112-53acb72157f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size of training dataset: 50000\n",
      "Size of validation dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "#Download CIFAR 10 dataset for training and validation purposes and apply the following changes on each image:\n",
    "# 1) make it a tensor\n",
    "# 2) normalize it based on the mean and standard deviation among all pixels in each channel (RGB).\n",
    "#Print the size of training and validation datasets\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
    "])\n",
    "\n",
    "# Download the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "print(\"Size of training dataset:\", len(train_dataset))\n",
    "print(\"Size of validation dataset:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944ca454-51fb-4b84-bab2-b6e57210248e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of subset training dataset: 307\n",
      "Size of subset validation dataset: 58\n"
     ]
    }
   ],
   "source": [
    "#We want to make a tertiary classifier that distinguishes between deers, dogs, and horses, labeled as 4, 5, and 7, resp.\n",
    "#Create the subset training and validation datasets for this purpose.\n",
    "#Print the size of these datasets.\n",
    "target_classes = [4, 5, 7]\n",
    "train_subset = torch.utils.data.Subset(train_dataset, [i for i in range(len(train_dataset)) if train_dataset[i][1] in target_classes])\n",
    "val_subset = torch.utils.data.Subset(val_dataset, [i for i in range(len(val_dataset)) if val_dataset[i][1] in target_classes])\n",
    "\n",
    "print(\"Size of subset training dataset:\", len(train_subset))\n",
    "print(\"Size of subset validation dataset:\", len(val_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12097380-5088-40f2-b608-f77b29cbe253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a parameterized CNN with the following details. \n",
    "# The parameter is the number of output channels n after the first convolution.\n",
    "# All kernels are of size 3 by 3.\n",
    "# Convolutions must not change the height and width.\n",
    "# Each convolution is followed by hyperbolic tangent as the activation function, and max pooling of size 2 by 2.\n",
    "# Convolution ayers:\n",
    "# 1) First convolution layer works on the input RGB input. Let's assume there are n kernels in this layer.\n",
    "# 2) Second convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer.\n",
    "# 3) Third convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer. \n",
    "# Fully connected layers:\n",
    "# 1) First fully connected layer works on the result of the preceding max pooling layer. \n",
    "#    This layer is followed by hyperbolic tangent as its activation function.\n",
    "# 2) Second fully connected layer works on the result of the preceding activation function, and emits numbers associated\n",
    "#    with each class.\n",
    "# We will use negative log likelihood to compute the loss. So you may add additional layer(s) to your network.\n",
    "# Note: Since the network is parameterized (n), you'd rather define the CNN as a subclass of nn.Module.\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, n, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, n, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(n, n // 2, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(n // 2, n // 2, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.Tanh()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(n // 2 * 4 * 4, n // 2)  # Assuming input size of 32x32\n",
    "        self.relu4 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(n // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.pool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        # Flatten the feature maps\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "n = 64  # You can change this value as needed\n",
    "num_classes = 10  # Adjust this based on your classification task\n",
    "model = CNN(n, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98377f6f-ceb7-4e48-af8b-e36e889494f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model with n = 16: 3314\n",
      "Total parameters in the model with n = 32: 12122\n"
     ]
    }
   ],
   "source": [
    "#Create two networks as instances of the CNN you defined above, with n = 16 and n = 32 respectively. \n",
    "#Print the total number of parameters in each of these instances.\n",
    "# Define the CNN class (as shown in the previous response)\n",
    "\n",
    "n1 = 16\n",
    "model1 = CNN(n1, num_classes)\n",
    "\n",
    "n2 = 32\n",
    "model2 = CNN(n2, num_classes)\n",
    "\n",
    "# Calculate and print the number of parameters for each model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "total_params_model1 = count_parameters(model1)\n",
    "total_params_model2 = count_parameters(model2)\n",
    "\n",
    "print(f\"Total parameters in the model with n = 16: {total_params_model1}\")\n",
    "print(f\"Total parameters in the model with n = 32: {total_params_model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83bf1a97-3934-4615-be2b-a71d5c6351bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training data loader: 32\n",
      "Number of batches in validation data loader: 7\n"
     ]
    }
   ],
   "source": [
    "#Our training functionality is supposed to compute gradient on batches of training data, randlomy selected each time.\n",
    "#To this end, create a training data loader with batch size 32 that randomizes access to each batch.\n",
    "#Also, create a validation data loader with the same batch size that does not randomize access to each batch (no need!)\n",
    "#Print the number of batches in training and validation data loaders\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a custom dataset for your training and validation data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Assuming you have your training and validation data and labels in tensors.\n",
    "train_data = torch.randn(1000, 3, 32, 32)  # Example random data for training\n",
    "train_labels = torch.randint(0, 10, (1000,))  # Example random labels for training\n",
    "\n",
    "val_data = torch.randn(200, 3, 32, 32)  # Example random data for validation\n",
    "val_labels = torch.randint(0, 10, (200,))  # Example random labels for validation\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(train_data, train_labels)\n",
    "val_dataset = CustomDataset(val_data, val_labels)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "\n",
    "# Training data loader with random shuffling of batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation data loader without random shuffling\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the number of batches in the training and validation data loaders\n",
    "print(f\"Number of batches in training data loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation data loader: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7c3ce2-0e4f-4c0f-97b7-9688342cd91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your training function that receives the training loader, model, loss function, optimizer, the device (cpu/gpu), and \n",
    "# number of epochs.\n",
    "#In each epoch, you should go through each training data batch, and:\n",
    "# 1) move data to device\n",
    "# 1) compute the output batch, and accordingly the loss\n",
    "# 2) compute the gradient of loss wrt parameters, and update the parameters\n",
    "#After covering all epochs, your training function must report the training accuracy\n",
    "import torch\n",
    "\n",
    "def train(model, train_loader, loss_fn, optimizer, device, num_epochs):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            # Move data to the device\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(outputs, batch_labels)\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += batch_labels.size(0)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate and print training accuracy for this epoch\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9b3b406-f7ac-4953-91a7-2a9a9fc7a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a separate function that receives the validation data loader as well as the model and computes the validation \n",
    "# accuracy of the model.\n",
    "import torch\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in val_loader:\n",
    "            # Move data to the device\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_data)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += batch_labels.size(0)\n",
    "            total_correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    return accuracy\n",
    "    accuracy = validate(model, val_loader, device)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96649fd1-8fbf-48e7-878b-f64e1bb55356",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m model_n16 \u001b[38;5;241m=\u001b[39m CNN(\u001b[38;5;241m16\u001b[39m)  \u001b[38;5;66;03m# Create a 16-n CNN model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()  \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model_n16\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mlearning_rate\u001b[49m)  \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m train(trainloader, model_n16, criterion, optimizer, device, epochs) \u001b[38;5;66;03m# Update parameters, calculate loss, and train the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m validate(testloader, model_n16, device) \u001b[38;5;66;03m# Utilize data validation to validate the training model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "#Define device dynamically based on whether CUDA is available or not.\n",
    "#Call the training function on the created training data loader, the created CNN  with n = 16, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "#Define the device (CPU in the absence of a GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#16-node model training and validation.\n",
    "model_n16 = CNN(16)  # Create a 16-n CNN model\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.SGD(model_n16.parameters(), lr=learning_rate)  # Optimizer\n",
    "\n",
    "train(trainloader, model_n16, criterion, optimizer, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate(testloader, model_n16, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc414df-8740-4196-97f2-7390f801631a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (456803862.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    validate_model(testloader, mo#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32.\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#Call the training function on the created training data loader, the created CNN  with n = 32, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why? \n",
    "# (This can be compared to the fully connected network we created in the last set of exercises.)\n",
    "\n",
    "model_n32 = CNN(32) # Make a 32-n CNN model\n",
    "optimizer = optim.SGD(model_n32.parameters(), lr=learning_rate)  # Optimizer\n",
    "train_model(trainloader, model_n32, criterion, optimizer, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, mo#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32. \n",
    "# Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "weight_decay = 0.002 # Decrease in weight in L2 regularization\n",
    "optimizer_l2 = optim.SGD(model_n32.parameters(), lr=learning_rate, weight_decay=weight_decay)  # an L2 regularized optimizer\n",
    "train_model(trainloader, model_n32, criterion, optimizer_l2, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_n32, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b74d1-9eb9-4886-ab0d-04b30f513416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32. \n",
    "# Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "weight_decay = 0.002 # Decrease in weight in L2 regularization\n",
    "optimizer_l2 = optim.SGD(model_n32.parameters(), lr=learning_rate, weight_decay=weight_decay)  # an L2 regularized optimizer\n",
    "train_model(trainloader, model_n32, criterion, optimizer_l2, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_n32, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c6d37b7-5efd-41a4-8cd3-aa84d81d7951",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SkipeCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m             num_features \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m s\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m num_features\n\u001b[1;32m---> 31\u001b[0m model_skip \u001b[38;5;241m=\u001b[39m \u001b[43mSkipCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Model a 32-n SkipCNN\u001b[39;00m\n\u001b[0;32m     32\u001b[0m optimizer_skip \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model_skip\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate) \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     33\u001b[0m train_model(trainloader, model_skip, criterion, optimizer_skip, device, epochs) \u001b[38;5;66;03m# Update parameters, calculate loss, and train the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36mSkipCNN.__init__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28msuper\u001b[39m(\u001b[43mSkipeCNN\u001b[49m, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m3\u001b[39m, n, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# 3 input channels, n output channels, and a 3x3 kernel are used in the convolutional layer\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(n, n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# n input channels, n//2 output channels, and a 3x3 kernel are used in the convolutional layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SkipeCNN' is not defined"
     ]
    }
   ],
   "source": [
    "#Add a skip connection in your CNN from the output of second max pooling to the input of 3rd max pooling.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "class SkipCNN(nn.Module):\n",
    "    # skip connections modified CNN model\n",
    "    def __init__(self, n):\n",
    "        super(SkipeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, n, 3) # 3 input channels, n output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.conv2 = nn.Conv2d(n, n//2, 3) # n input channels, n//2 output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.conv3 = nn.Conv2d(n//2, n//2, 3) # n//2 input channels, n//2 output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.fc1 = nn.Linear(n//2 * 2 * 2, n//2) # n//2 * 2 * 2 feature mappings to n//2 neurons are flattened by the first connected layer\n",
    "        self.fc2 = nn.Linear(n//2, len(class_indices)) # n//2 input neurons, output neurons based on class indices\n",
    "        self.activation = nn.Tanh() # Hyperbolic Tangent is the activation function (Tanh)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 kernel, stride 2, maximum pooling layer\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool(self.activation(self.conv1(x))) # Convolution, activation, and pooling are applied to the first layer\n",
    "        x2 = self.pool(self.activation(self.conv2(x1))) # Convolution, activation, and pooling are applied to the second layer\n",
    "        x3 = self.pool(self.activation(self.conv3(x2))) # Convolution, activation, and pooling are applied to the third layer\n",
    "        x = x3.view(-1, self.num_flat_features(x3)) # Tensor flattening for connected layers\n",
    "        x = self.activation(self.fc1(x)) # activating the top layer that is connected\n",
    "        x = self.fc2(x) # Applied a second connected layer.\n",
    "        return x\n",
    "    def num_flat_features(self, x): # Total the features in a tensor without using the batch dimension\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "model_skip = SkipCNN(32) # Model a 32-n SkipCNN\n",
    "optimizer_skip = optim.SGD(model_skip.parameters(), lr=learning_rate) # Optimizer\n",
    "train_model(trainloader, model_skip, criterion, optimizer_skip, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_skip, device) #  Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db51f6c-5848-4a61-a85b-0c8ff7809582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m             num_features \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m s\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m num_features\n\u001b[1;32m---> 38\u001b[0m model_dropout \u001b[38;5;241m=\u001b[39m \u001b[43mDropoutCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Create a 32-n DropoutCNN model.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m optimizer_dropout \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model_dropout\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate) \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[0;32m     40\u001b[0m train_model(trainloader, model_dropout, criterion, optimizer_dropout, device, epochs) \u001b[38;5;66;03m# Update parameters, calculate loss, and train the model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m, in \u001b[0;36mDropoutCNN.__init__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# 'n//2' input channles, 'n//2' output channels, and 3 kernel sizes make up the third convulutional layer\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# With input characteristics of \"n//2 * 2 * 2\" and output features of \"n//2,\" the first connected linear layer\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mclass_indices\u001b[49m)) \u001b[38;5;66;03m# Create a linear layer with n//2 input features and class_indices length output features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTanh() \u001b[38;5;66;03m# Make the activation function's initial value a hyperbolic tangent (Tanh)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Make a maximum 2x2 pooling layer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_indices' is not defined"
     ]
    }
   ],
   "source": [
    "#Consider dropout layers after each max pooling in the original CNN, where the probability of zeroing output features is 30%.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "#From nn.Module, create the DropoutCNN neural network class.\n",
    "class DropoutCNN(nn.Module):\n",
    "    # The 'n' input in the class constructor denotes the number of channels in a convolutional layer.\n",
    "    def __init__(self, n):\n",
    "        super(DropoutCNN, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(3, n, 3) # 3 input channels, 'n' output channels, and 3 kernel sizes make up the first convolutional layer\n",
    "        self.conv2 = nn.Conv2d(n, n//2, 3) # 'n' input channels, 'n//2' output channles, and 3 kernel sizes make up the second convolutional layer\n",
    "        self.conv3 = nn.Conv2d(n//2, n//2, 3) # 'n//2' input channles, 'n//2' output channels, and 3 kernel sizes make up the third convulutional layer\n",
    "        self.fc1 = nn.Linear(n//2 * 2 * 2, n//2) # With input characteristics of \"n//2 * 2 * 2\" and output features of \"n//2,\" the first connected linear layer\n",
    "        self.fc2 = nn.Linear(n//2, len(class_indices)) # Create a linear layer with n//2 input features and class_indices length output features\n",
    "        self.activation = nn.Tanh() # Make the activation function's initial value a hyperbolic tangent (Tanh)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Make a maximum 2x2 pooling layer\n",
    "        self.dropout = nn.Dropout(p=0.3) # Create a layer with a 0.3% dropout rate\n",
    "\n",
    "    # Network forward pass is defined by the forward method\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x))) # Applying the first convolutional layer, then activating and max pooling\n",
    "        x = self.dropout(x) # To the output, apply dropout\n",
    "        x = self.pool(self.activation(self.conv2(x))) # Applying the second convolutional layer, then activating and max pooling\n",
    "        x = self.dropout(x) # To the output, apply dropout\n",
    "        x = self.pool(self.activation(self.conv3(x))) # Applying the third convolutional layer, then activating and max pooling\n",
    "        x = x.view(-1, self.num_flat_features(x)) # Connected layer shape tensor\n",
    "        x = self.activation(self.fc1(x)) # Activate the output of the first connected layer\n",
    "        x = self.fc2(x) # apply the connected second layer\n",
    "        return x # Return the final tensor\n",
    "    \n",
    "    def num_flat_features(self, x): # Total the features in a tensor without using the batch dimension\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model_dropout = DropoutCNN(32) # Create a 32-n DropoutCNN model.\n",
    "optimizer_dropout = optim.SGD(model_dropout.parameters(), lr=learning_rate) # Optimizer\n",
    "train_model(trainloader, model_dropout, criterion, optimizer_dropout, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_dropout, device) #  Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec666c8d-25b3-4135-b284-ef3361cee8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering all the modifications which one works better? Plain CNN, CNN+L2, CNN+Skip, CNN+Dropout?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
