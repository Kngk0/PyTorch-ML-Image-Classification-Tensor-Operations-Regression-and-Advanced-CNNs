{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0579-9bcf-4f0e-b222-05c78ef37ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download CIFAR 10 dataset for training and validation purposes and apply the following changes on each image:\n",
    "# 1) make it a tensor\n",
    "# 2) normalize it based on the mean and standard deviation among all pixels in each channel (RGB).\n",
    "#Print the size of training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90374899-4072-4ee4-a3c8-a1b0ed19ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make a tertiary classifier that distinguishes between deers, dogs, and horses, labeled as 4, 5, and 7, resp.\n",
    "#Create the subset training and validation datasets for this purpose.\n",
    "#Print the size of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ff49-791b-4e63-9d35-35411a0b8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a parameterized CNN with the following details. \n",
    "# The parameter is the number of output channels n after the first convolution.\n",
    "# All kernels are of size 3 by 3.\n",
    "# Convolutions must not change the height and width.\n",
    "# Each convolution is followed by hyperbolic tangent as the activation function, and max pooling of size 2 by 2.\n",
    "# Convolution ayers:\n",
    "# 1) First convolution layer works on the input RGB input. Let's assume there are n kernels in this layer.\n",
    "# 2) Second convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer.\n",
    "# 3) Third convolution layer works on the result of the preceding max pooling layer. \n",
    "#    Let's assume there are n/2 kernels in this layer. \n",
    "# Fully connected layers:\n",
    "# 1) First fully connected layer works on the result of the preceding max pooling layer. \n",
    "#    This layer is followed by hyperbolic tangent as its activation function.\n",
    "# 2) Second fully connected layer works on the result of the preceding activation function, and emits numbers associated\n",
    "#    with each class.\n",
    "# We will use negative log likelihood to compute the loss. So you may add additional layer(s) to your network.\n",
    "# Note: Since the network is parameterized (n), you'd rather define the CNN as a subclass of nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be13802-cee5-4150-b78c-49fae49d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two networks as instances of the CNN you defined above, with n = 16 and n = 32 respectively. \n",
    "#Print the total number of parameters in each of these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7def331-3852-4cc7-9c97-52a1d5831523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our training functionality is supposed to compute gradient on batches of training data, randlomy selected each time.\n",
    "#To this end, create a training data loader with batch size 32 that randomizes access to each batch.\n",
    "#Also, create a validation data loader with the same batch size that does not randomize access to each batch (no need!)\n",
    "#Print the number of batches in training and validation data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ef16c-813a-459a-a65c-7d710321c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your training function that receives the training loader, model, loss function, optimizer, the device (cpu/gpu), and \n",
    "# number of epochs.\n",
    "#In each epoch, you should go through each training data batch, and:\n",
    "# 1) move data to device\n",
    "# 1) compute the output batch, and accordingly the loss\n",
    "# 2) compute the gradient of loss wrt parameters, and update the parameters\n",
    "#After covering all epochs, your training function must report the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfa4c0-173f-4afd-87ee-e602276f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a separate function that receives the validation data loader as well as the model and computes the validation \n",
    "# accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e6166-15b7-4a07-aab5-eebbb2acc386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define device dynamically based on whether CUDA is available or not.\n",
    "#Call the training function on the created training data loader, the created CNN  with n = 16, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "#Define the device (CPU in the absence of a GPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#16-node model training and validation.\n",
    "model_n16 = CNN(16)  # Create a 16-n CNN model\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "optimizer = optim.SGD(model_n16.parameters(), lr=learning_rate)  # Optimizer\n",
    "\n",
    "train(trainloader, model_n16, criterion, optimizer, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate(testloader, model_n16, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f930c-039d-4123-b90d-60fac6d68afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the training function on the created training data loader, the created CNN  with n = 32, \n",
    "# negative log likelihood loss function, stochastic gradient descent optimizer,\n",
    "# the device you defined, and 100 epochs. Next, call validation accuracy function.\n",
    "#Is the model overfit? (Yes/No) Why? \n",
    "# (This can be compared to the fully connected network we created in the last set of exercises.)\n",
    "\n",
    "model_n32 = CNN(32) # Make a 32-n CNN model\n",
    "optimizer = optim.SGD(model_n32.parameters(), lr=learning_rate)  # Optimizer\n",
    "train_model(trainloader, model_n32, criterion, optimizer, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_n32, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dde7111-bf9d-4a9b-b5df-f782586e60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, let's consider L2 regularization with weight decay 0.002 for CNN with n = 32. \n",
    "# Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "weight_decay = 0.002 # Decrease in weight in L2 regularization\n",
    "optimizer_l2 = optim.SGD(model_n32.parameters(), lr=learning_rate, weight_decay=weight_decay)  # an L2 regularized optimizer\n",
    "train_model(trainloader, model_n32, criterion, optimizer_l2, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_n32, device) # Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e315a-0f42-4dbf-b641-6c11fbe206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a skip connection in your CNN from the output of second max pooling to the input of 3rd max pooling.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "class SkipCNN(nn.Module):\n",
    "    # skip connections modified CNN model\n",
    "    def __init__(self, n):\n",
    "        super(SkipeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, n, 3) # 3 input channels, n output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.conv2 = nn.Conv2d(n, n//2, 3) # n input channels, n//2 output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.conv3 = nn.Conv2d(n//2, n//2, 3) # n//2 input channels, n//2 output channels, and a 3x3 kernel are used in the convolutional layer\n",
    "        self.fc1 = nn.Linear(n//2 * 2 * 2, n//2) # n//2 * 2 * 2 feature mappings to n//2 neurons are flattened by the first connected layer\n",
    "        self.fc2 = nn.Linear(n//2, len(class_indices)) # n//2 input neurons, output neurons based on class indices\n",
    "        self.activation = nn.Tanh() # Hyperbolic Tangent is the activation function (Tanh)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 kernel, stride 2, maximum pooling layer\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool(self.activation(self.conv1(x))) # Convolution, activation, and pooling are applied to the first layer\n",
    "        x2 = self.pool(self.activation(self.conv2(x1))) # Convolution, activation, and pooling are applied to the second layer\n",
    "        x3 = self.pool(self.activation(self.conv3(x2))) # Convolution, activation, and pooling are applied to the third layer\n",
    "        x = x3.view(-1, self.num_flat_features(x3)) # Tensor flattening for connected layers\n",
    "        x = self.activation(self.fc1(x)) # activating the top layer that is connected\n",
    "        x = self.fc2(x) # Applied a second connected layer.\n",
    "        return x\n",
    "    def num_flat_features(self, x): # Total the features in a tensor without using the batch dimension\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "model_skip = SkipCNN(32) # Model a 32-n SkipCNN\n",
    "optimizer_skip = optim.SGD(model_skip.parameters(), lr=learning_rate) # Optimizer\n",
    "train_model(trainloader, model_skip, criterion, optimizer_skip, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_skip, device) #  Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebfaac-1a04-4695-aa15-c83f86290c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consider dropout layers after each max pooling in the original CNN, where the probability of zeroing output features is 30%.\n",
    "#Train the updated CNN with the same parameters including (n = 32).\n",
    "#Is the model overfit? (Yes/No) Why?\n",
    "\n",
    "#From nn.Module, create the DropoutCNN neural network class.\n",
    "class DropoutCNN(nn.Module):\n",
    "    # The 'n' input in the class constructor denotes the number of channels in a convolutional layer.\n",
    "    def __init__(self, n):\n",
    "        super(DropoutCNN, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(3, n, 3) # 3 input channels, 'n' output channels, and 3 kernel sizes make up the first convolutional layer\n",
    "        self.conv2 = nn.Conv2d(n, n//2, 3) # 'n' input channels, 'n//2' output channles, and 3 kernel sizes make up the second convolutional layer\n",
    "        self.conv3 = nn.Conv2d(n//2, n//2, 3) # 'n//2' input channles, 'n//2' output channels, and 3 kernel sizes make up the third convulutional layer\n",
    "        self.fc1 = nn.Linear(n//2 * 2 * 2, n//2) # With input characteristics of \"n//2 * 2 * 2\" and output features of \"n//2,\" the first connected linear layer\n",
    "        self.fc2 = nn.Linear(n//2, len(class_indices)) # Create a linear layer with n//2 input features and class_indices length output features\n",
    "        self.activation = nn.Tanh() # Make the activation function's initial value a hyperbolic tangent (Tanh)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # Make a maximum 2x2 pooling layer\n",
    "        self.dropout = nn.Dropout(p=0.3) # Create a layer with a 0.3% dropout rate\n",
    "\n",
    "    # Network forward pass is defined by the forward method\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x))) # Applying the first convolutional layer, then activating and max pooling\n",
    "        x = self.dropout(x) # To the output, apply dropout\n",
    "        x = self.pool(self.activation(self.conv2(x))) # Applying the second convolutional layer, then activating and max pooling\n",
    "        x = self.dropout(x) # To the output, apply dropout\n",
    "        x = self.pool(self.activation(self.conv3(x))) # Applying the third convolutional layer, then activating and max pooling\n",
    "        x = x.view(-1, self.num_flat_features(x)) # Connected layer shape tensor\n",
    "        x = self.activation(self.fc1(x)) # Activate the output of the first connected layer\n",
    "        x = self.fc2(x) # apply the connected second layer\n",
    "        return x # Return the final tensor\n",
    "    \n",
    "    def num_flat_features(self, x): # Total the features in a tensor without using the batch dimension\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "model_dropout = DropoutCNN(32) # Create a 32-n DropoutCNN model.\n",
    "optimizer_dropout = optim.SGD(model_dropout.parameters(), lr=learning_rate) # Optimizer\n",
    "train_model(trainloader, model_dropout, criterion, optimizer_dropout, device, epochs) # Update parameters, calculate loss, and train the model\n",
    "validate_model(testloader, model_dropout, device) #  Utilize data validation to validate the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33917fa1-51d7-42cb-808d-7caa06ef8908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering all the modifications which one works better? Plain CNN, CNN+L2, CNN+Skip, CNN+Dropout?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
