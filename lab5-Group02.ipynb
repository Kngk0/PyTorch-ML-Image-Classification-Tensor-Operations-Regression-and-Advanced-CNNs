{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a442a9a8-b49e-4101-a382-2e3342d7c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input and output vectors are given. \n",
    "inp = [ 0.7300, -1.0400, -1.2300,  1.6700, -0.6300,  1.4300, -0.8400,  0.1500,\n",
    "         -2.3000,  3.1000, -1.4500, -1.8100,  1.8700, -0.1100, -0.2800,  1.1200,\n",
    "         -0.4200,  2.8900]\n",
    "out = [ 1.43,  10.1,  8.3,  1.03,  10.21, -0.1,  8.92,  5.1,\n",
    "         -7.53, 34.72,  7.61,  3.2,  2.19,  7.15,  7.69, -0.18,\n",
    "          8.81, 23.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07978b6d-94d0-473b-a137-7199551249a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the training and validation datasets: 80% training, 20% validation\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "inp_tensor = torch.tensor(inp, dtype=torch.float32).reshape(-1,1)\n",
    "out_tensor = torch.tensor(out, dtype=torch.float32).reshape(-1,1)\n",
    "\n",
    "# split data into training and validation sets\n",
    "train_size = int(0.8 * len(inp_tensor))\n",
    "train_data, val_data = inp_tensor[:train_size], inp_tensor[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa9e83f-8cf1-4ef0-bede-e6356e16634c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Create a NN that consists of:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# a linear layer of input size 1 and output size 15, followed by hyperbolic tangent as its activation function\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# a linear layer of input size 15 and output size 23, followed by hyperbolic tangent as its activation function\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# a linear layer of input size 23 and output size 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# define the newural network\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      8\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m      9\u001b[0m     nn\u001b[38;5;241m.\u001b[39mTanh(),\n\u001b[1;32m     10\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m23\u001b[39m),\n\u001b[1;32m     11\u001b[0m     nn\u001b[38;5;241m.\u001b[39mTanh(),\n\u001b[1;32m     12\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Write a little script that shows the number of parameters in each layer.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Based on the output of this script, report as a comment in your code how many weights and biases exist in each layer.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a NN that consists of:\n",
    "# a linear layer of input size 1 and output size 15, followed by hyperbolic tangent as its activation function\n",
    "# a linear layer of input size 15 and output size 23, followed by hyperbolic tangent as its activation function\n",
    "# a linear layer of input size 23 and output size 1\n",
    "\n",
    "# define the newural network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1,15),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(15,23),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(23,1),\n",
    ")\n",
    "\n",
    "# Write a little script that shows the number of parameters in each layer.\n",
    "# Based on the output of this script, report as a comment in your code how many weights and biases exist in each layer.\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"layer '{name}' has {param.numel()}.\")\n",
    "# Next, define the training function that receives training and validation datasets, along with a model, loss function, \n",
    "# optimizer, and number of epochs. The function must use the model's own parameter handling and the the input loss function\n",
    "# to automatically calculate the gradient of the loss wrt parameters (autograd), and use optimizer to update the parameters \n",
    "# and zero the gradients. \n",
    "\n",
    "# training function\n",
    "def train(train_data, val_data, model, loss_function, optimizer, epochs):\n",
    "    train_losses = [] # list to store training losses for each epoch\n",
    "    val_losses = []   # list to store validation losses for each epoch\n",
    "\n",
    "    # training loop for the epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # set the model to training mode\n",
    "        optimizer.zero_grad() # zero out gradients from previous iteration\n",
    "        train_outputs = model(train_data) # compute predicted outputs\n",
    "        loss = loss_function(train_outputs, train_data) # compute loss\n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # update weights and biases using gradients\n",
    "        train_losses.append(loss.item()) # store training loss for this epoch\n",
    "\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad(): \n",
    "            val_outputs = model(val_data) # forward pass on validation data\n",
    "            val_loss = loss_function(val_outputs, val_data) # compute validation loss\n",
    "            val_losses.append(val_loss.item()) # store validation loss for this epoch\n",
    "\n",
    "        # print progress for current epoch\n",
    "        print(f\"Epoch: [{epoch+1}/{epochs}], \" f\"Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses # return lists of training and validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3b3b7ea-9e5a-4326-ac0a-dc6ae8f62da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your NN with built-in mean square error loss function and SGD optimizer. \n",
    "# Try different learning rates and number of epochs improve the results.\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb438eaa-34cc-43d6-9057-1e982e12ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
