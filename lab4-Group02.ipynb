{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069e89fa-8aa8-478c-8d9d-c100df43b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output vectors are given. \n",
    "inp = [ 0.7300, -1.0400, -1.2300,  1.6700, -0.6300,  1.4300, -0.8400,  0.1500,\n",
    "         -2.3000,  3.1000, -1.4500, -1.8100,  1.8700, -0.1100, -0.2800,  1.1200,\n",
    "         -0.4200,  2.8900]\n",
    "out = [ 1.43,  10.1,  8.3,  1.03,  10.21, -0.1,  8.92,  5.1,\n",
    "         -7.53, 34.72,  7.61,  3.2,  2.19,  7.15,  7.69, -0.18,\n",
    "          8.81, 23.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cd7d2d-2354-4415-945e-706b1dafebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the polynomial model of degree 3, i.e., having 3 weights and 1 bias. \n",
    "# Also define the loss function\n",
    "def polynomial_model(x, weight1, weight2, weight3, bias):\n",
    "    return weight1 * x**3 + weight2 * x**2 + weight3 * x + bias\n",
    "\n",
    "def loss(predictions, targets):\n",
    "    return sum((predictions - targets)**2) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff844417-2447-4cd8-92dc-92c608f3fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient manually wrt the exisiting parameters\n",
    "# Note: You need to define appropriate derivative functions to define the gradient\n",
    "# Use the defined gradient function to define the training function \n",
    "# Note: You cannot use autograd and optimizers\n",
    "# Run it on the input and output vector with appropriate learning rate and number of iterations\n",
    "# Plot the learned curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268934b-6862-422d-9960-a9e30091edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Define the training function\n",
    "# Note: You cannot use optimizers.\n",
    "# Run it on the input and output vector with appropriate learning rate and number of iterations\n",
    "# Plot the learned curve\n",
    "import torch\n",
    "\n",
    "inp = torch.tensor([ 0.7300, -1.0400, -1.2300,  1.6700, -0.6300,  1.4300, -0.8400,  0.1500,\n",
    "                    -2.3000,  3.1000, -1.4500, -1.8100,  1.8700, -0.1100, -0.2800,  1.1200,\n",
    "                    -0.4200,  2.8900], requires_grad=True)\n",
    "out = torch.tensor([ 1.43,  10.1,  8.3,  1.03,  10.21, -0.1,  8.92,  5.1,\n",
    "                    -7.53, 34.72,  7.61,  3.2,  2.19,  7.15,  7.69, -0.18,\n",
    "                    8.81, 23.1], requires_grad=False)\n",
    "def model(x, w, b):\n",
    "    return w * x + b\n",
    "    \n",
    "def mse_loss(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "    \n",
    "def train(x, y, epochs, lr):\n",
    "    w = torch.randn(1, requires_grad=True)\n",
    "    b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(x, w, b)\n",
    "        loss = mse_loss(y_pred, y)\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Manual gradient descent\n",
    "        with torch.nograd():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "            # Zero gradients\n",
    "            w.grad.zero()\n",
    "            b.grad.zero_()\n",
    "\n",
    "    return w, b\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "w, b = train(inp, out, epochs, learning_rate)\n",
    "\n",
    "predictions = model(inp, w, b).detach().numpy()\n",
    "\n",
    "plt.scatter(inp.numpy(), out.numpy(), color='blue', label='Actual data')\n",
    "plt.plot(inp.numpy(), predictions, color='red', label='Fitted line')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Linear Regression using Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8d657-4aef-42a0-806e-0f3fa5c8d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Use optimizers to abstract how parameters get updated\n",
    "# Define the training function\n",
    "# Run it on the input and output vector with appropriate learning rate, number of iterations, and SGD optimizer\n",
    "# Plot the learned curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961f2ff-d345-4ef6-98ab-047ed9f4411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to training set (80%) and validation set (20%) randomly\n",
    "# Use PyTorch's autograd to automatically compute the gradients \n",
    "# Use optimizers to abstract how parameters get updated\n",
    "# Define the training function that tracks both training and validation losses\n",
    "# Run it on the input and output vector with appropriate learning rate, number of iterations, and SGD optimizer\n",
    "# Plot the learned curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "inp = torch.tensor([ 0.7300, -1.0400, -1.2300,  1.6700, -0.6300,  1.4300, -0.8400,  0.1500,\n",
    "                    -2.3000,  3.1000, -1.4500, -1.8100,  1.8700, -0.1100, -0.2800,  1.1200,\n",
    "                    -0.4200,  2.8900], dtype=torch.float32).view(-1, 1)\n",
    "out = torch.tensor([ 1.43,  10.1,  8.3,  1.03,  10.21, -0.1,  8.92,  5.1,\n",
    "                    -7.53, 34.72,  7.61,  3.2,  2.19,  7.15,  7.69, -0.18,\n",
    "                    8.81, 23.1], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "dataset = torch.utils.data.TensorDataset(inp, out)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_size)\n",
    "\n",
    "# Model definition\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Training function\n",
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        # Training loop\n",
    "        for x_train, y_train in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_train = model(x_train)\n",
    "            loss_train = criterion(y_pred_train, y_train)\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss_train.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                y_pred_val = model(x_val)\n",
    "                loss_val = criterion(y_pred_val, y_val)\n",
    "\n",
    "                val_losses.append(loss_val.item())\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = LinearRegression()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train\n",
    "epochs = 1000\n",
    "train_losses, val_losses = train(model, criterion, optimizer, train_loader, val_loader, epochs)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    predictions = model(inp)\n",
    "\n",
    "# Plot learned curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(inp.numpy(), out.numpy(), color='blue', label='Actual data')\n",
    "plt.plot(inp.numpy(), predictions.numpy(), color='red', label='Fitted line')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Linear Regression using SGD Optimizer')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
